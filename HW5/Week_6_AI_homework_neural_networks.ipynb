{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUbEmuvZJxlI"
      },
      "source": [
        "# PyTorch - homework 2: neural networks\n",
        "\n",
        "-- Prof. Dorien Herremans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efS07mO7J6AR"
      },
      "source": [
        "Please run the whole notebook with your code and submit the `.ipynb` file on eDimension that includes your answers [so after you run it]. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "mJpzFaX0J6Zz",
        "outputId": "33e49695-d65f-4948-f0ea-057c6f3ad787"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mHomework by Victoria Yong, number: 1004455\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from termcolor import colored\n",
        "\n",
        "student_number=\"1004455\"\n",
        "student_name=\"Victoria Yong\"\n",
        "\n",
        "print(colored(\"Homework by \"  + student_name + ', number: ' + student_number,'red'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xDkwBg8LKQ_"
      },
      "source": [
        " ## Question 1 -- XOR neural network [3pts]\n",
        "\n",
        "a) Train an (at least) 2-layer neural network that can solve the XOR problem. Hint: be sure to check both this week and last week's lab. \n",
        "\n",
        "b) Check the predictions resulting from your model in the second code box below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BINvhm-PLKak"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 1.9813592433929443, \n",
            "Epoch: 100, Loss: 0.7987769246101379, \n",
            "Epoch: 200, Loss: 0.4552419185638428, \n",
            "Epoch: 300, Loss: 0.40526947379112244, \n",
            "Epoch: 400, Loss: 0.34230825304985046, \n",
            "Epoch: 500, Loss: 0.32782965898513794, \n",
            "Epoch: 600, Loss: 0.3068409562110901, \n",
            "Epoch: 700, Loss: 0.19627150893211365, \n",
            "Epoch: 800, Loss: 0.2258622944355011, \n",
            "Epoch: 900, Loss: 0.14653223752975464, \n"
          ]
        }
      ],
      "source": [
        "# load your data\n",
        "feats = torch.Tensor([[0, 0],\n",
        "                        [0, 1],\n",
        "                        [1, 0],\n",
        "                        [1, 1]])\n",
        "labels = torch.Tensor([[0, 1, 1, 0]]).view(-1, 1)\n",
        "\n",
        "in_dim = feats.size(1)\n",
        "\n",
        "# name your model xor\n",
        "def xor(in_dim=4, out_dim=1):\n",
        "# define your model loss function, optimizer, etc. \n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(in_dim, 128),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(128, out_dim),\n",
        "        nn.Sigmoid()\n",
        "    ).cuda()\n",
        "\n",
        "# initialize weights\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            m.weight.data.normal_(0, 1)\n",
        "\n",
        "    return model\n",
        "\n",
        "#########################################################################################\n",
        "\n",
        "model = xor(in_dim)\n",
        "epochs = 1000\n",
        "\n",
        "loss_func = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# train the model\n",
        "for epoch in range(epochs):\n",
        "    for j in range(feats.size(0)):\n",
        "        idx = np.random.randint(feats.size(0))\n",
        "        x = torch.autograd.Variable(feats[idx], requires_grad=False).cuda()\n",
        "        y = torch.autograd.Variable(labels[idx], requires_grad=False).cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = model(x)\n",
        "        loss = loss_func(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch: {epoch}, Loss: {loss.cpu().data.numpy()}, \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "51Ra1T6n2r_R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 xor 0 = 0\n",
            "0 xor 1 = 1\n",
            "1 xor 1 = 0\n",
            "1 xor 0 = 1\n"
          ]
        }
      ],
      "source": [
        "# test your model using the following functions (make sure the output is printed and saved when you submit this notebook):\n",
        "# depending on how you defined your network you may need to slightly tweek the below prediction function\n",
        "\n",
        "test = [[0,0],[0,1],[1,1],[1,0]]\n",
        "\n",
        "for trial in test: \n",
        "  Xtest = torch.Tensor(trial).cuda()\n",
        "  y_hat = model(Xtest)\n",
        "\n",
        "  if y_hat > 0.5:\n",
        "    prediction = 1\n",
        "  else: \n",
        "    prediction = 0\n",
        "\n",
        "  print(\"{0} xor {1} = {2}\".format(int(Xtest[0]), int(Xtest[1]), prediction))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqIqD5ZzyUOW"
      },
      "source": [
        "## Question 2  [2pts]\n",
        "\n",
        "Imagine a neural network model for a multilabel classification task. \n",
        "\n",
        "a) Which loss function should you use?\n",
        "\n",
        "b) The resulting trained modal has a high variance error. Give 4 possible solutions to improve the model. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzye9G18PQ0c"
      },
      "source": [
        "```\n",
        "a) Binary Cross Entropy Loss\n",
        "\n",
        "b) \n",
        "- 1 : Add dropout layers\n",
        "- 2 : Use data augmentation on training data\n",
        "- 3 : Implement early stopping\n",
        "- 4 : Get more training data that is balanced across classes\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcceOSnjjSHf"
      },
      "source": [
        "## Question 3 - Improve hit classification [5pts]\n",
        "\n",
        "Remember the hit predicton dataset from last week? \n",
        "\n",
        "a) Improve the model using a multiplayer perceptron. \n",
        "\n",
        "b) Make sure to run your models on the GPU. \n",
        "\n",
        "c) Tweek the hyperparameters such as number of nodes or layers, or other. Show two possible configurations and explain which works better and very briefly explain why this may be the case. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_5996\\1418662058.py:7: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
            "  train_feats = train_data.drop('Topclass1030', 1)\n"
          ]
        }
      ],
      "source": [
        "# load train_data\n",
        "train_url = 'https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030training.csv'\n",
        "test_url = 'https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030test.csv'\n",
        "\n",
        "train_data = pd.read_csv(train_url)\n",
        "train_labels = train_data['Topclass1030']\n",
        "train_feats = train_data.drop('Topclass1030', 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t-jkJDTdjSRX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0\t Loss: 0.7091912031173706\n",
            "Epoch: 1\t Loss: 0.7062981128692627\n",
            "Epoch: 2\t Loss: 0.7014076113700867\n",
            "Epoch: 3\t Loss: 0.697381854057312\n",
            "Epoch: 4\t Loss: 0.6969487071037292\n",
            "Epoch: 5\t Loss: 0.7048768997192383\n",
            "Epoch: 6\t Loss: 0.7182644605636597\n",
            "Epoch: 7\t Loss: 0.7425717115402222\n",
            "Epoch: 8\t Loss: 0.7641268968582153\n",
            "Epoch: 9\t Loss: 0.7886040210723877\n",
            "Epoch: 10\t Loss: 0.8054547905921936\n",
            "Epoch: 11\t Loss: 0.8162824511528015\n",
            "Epoch: 12\t Loss: 0.8217505812644958\n",
            "Epoch: 13\t Loss: 0.8249359726905823\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\victo\\OneDrive\\Desktop\\temp\\50.021-AI-Assignments\\HW5\\Week_6_AI_homework_neural_networks.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/victo/OneDrive/Desktop/temp/50.021-AI-Assignments/HW5/Week_6_AI_homework_neural_networks.ipynb#ch0000010?line=33'>34</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/victo/OneDrive/Desktop/temp/50.021-AI-Assignments/HW5/Week_6_AI_homework_neural_networks.ipynb#ch0000010?line=34'>35</a>\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(train_data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/victo/OneDrive/Desktop/temp/50.021-AI-Assignments/HW5/Week_6_AI_homework_neural_networks.ipynb#ch0000010?line=35'>36</a>\u001b[0m         feats \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(train_feats\u001b[39m.\u001b[39;49mloc[j]\u001b[39m.\u001b[39;49mvalues)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/victo/OneDrive/Desktop/temp/50.021-AI-Assignments/HW5/Week_6_AI_homework_neural_networks.ipynb#ch0000010?line=36'>37</a>\u001b[0m         label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([train_labels\u001b[39m.\u001b[39mloc[j]])\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/victo/OneDrive/Desktop/temp/50.021-AI-Assignments/HW5/Week_6_AI_homework_neural_networks.ipynb#ch0000010?line=38'>39</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# code your model 1\n",
        "\n",
        "# MLP\n",
        "class MLP(nn.Module):\n",
        " \n",
        "  def __init__(self, input_size, num_classes):\n",
        "    super(MLP, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size, 64)\n",
        "    self.fc2 = nn.Linear(64, 16)\n",
        "    self.fc3 = nn.Linear(16, num_classes)\n",
        "   \n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.fc1(x)\n",
        "    out = torch.relu(out)\n",
        "    out = self.fc2(out)\n",
        "    out = torch.relu(out)\n",
        "    out = self.fc3(out)\n",
        "    out = torch.relu(out)\n",
        "    out = torch.sigmoid(out)\n",
        "    return out\n",
        "\n",
        "# train model\n",
        "num_outputs = 1\n",
        "num_input_features = train_data.shape[1] - 1\n",
        "model1 = MLP(num_input_features, num_outputs).cuda()\n",
        "\n",
        "lr_rate = 1e-4\n",
        "loss_function = nn.BCELoss() \n",
        "optimizer = torch.optim.SGD(model1.parameters(), lr=lr_rate)\n",
        "\n",
        "epochs = 200 \n",
        "\n",
        "for i in range(epochs):\n",
        "    for j in range(train_data.shape[0]):\n",
        "        feats = torch.tensor(train_feats.loc[j].values).float().cuda()\n",
        "        label = torch.tensor([train_labels.loc[j]]).float().cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = model1(feats)\n",
        "\n",
        "        loss = loss_function(pred, label).cuda() \n",
        "        loss.backward() \n",
        "        optimizer.step() \n",
        "\n",
        "    if i % 10 == 0:\n",
        "        print (f\"Epoch: {i}\\t Loss: {loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIDPTKcFkETc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True Positives: 34, True Negatives: 4\n",
            "False Positives: 25, False Negatives: 16\n",
            "Class specific accuracy of correctly predicting a hit song is 0.68\n"
          ]
        }
      ],
      "source": [
        "# evaluate model 1 (called model1 here)\n",
        "import pandas as pd \n",
        "\n",
        "def run_evaluation(my_model):\n",
        "\n",
        "  test = pd.read_csv(test_url)\n",
        "  labels = test.iloc[:,-1]\n",
        "  test = test.drop('Topclass1030', axis=1)\n",
        "  testdata = torch.Tensor(test.values)\n",
        "  testlabels = torch.Tensor(labels.values).view(-1,1).cuda()\n",
        "\n",
        "  TP = 0\n",
        "  TN = 0\n",
        "  FN = 0\n",
        "  FP = 0\n",
        "\n",
        "  for i in range(0, testdata.size()[0]): \n",
        "    # print(testdata[i].size())\n",
        "    Xtest = torch.Tensor(testdata[i]).cuda()\n",
        "    y_hat = my_model(Xtest)\n",
        "    \n",
        "    if y_hat > 0.5:\n",
        "      prediction = 1\n",
        "    else: \n",
        "      prediction = 0\n",
        "\n",
        "    if (prediction == testlabels[i]):\n",
        "      if (prediction == 1):\n",
        "        TP += 1\n",
        "      else: \n",
        "        TN += 1\n",
        "\n",
        "    else:\n",
        "      if (prediction == 1):\n",
        "        FP += 1\n",
        "      else: \n",
        "        FN += 1\n",
        "\n",
        "  print(\"True Positives: {0}, True Negatives: {1}\".format(TP, TN))\n",
        "  print(\"False Positives: {0}, False Negatives: {1}\".format(FP, FN))\n",
        "  rate = TP/(FN+TP)\n",
        "  print(\"Class specific accuracy of correctly predicting a hit song is {0}\".format(rate))\n",
        "\n",
        "run_evaluation(model1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xghPDDNmkHn2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0\t Loss: 0.8215988874435425\n",
            "Epoch: 1\t Loss: 0.6931471824645996\n",
            "Epoch: 2\t Loss: 0.6931471824645996\n",
            "Epoch: 3\t Loss: 0.6931471824645996\n",
            "Epoch: 4\t Loss: 0.7581603527069092\n",
            "Epoch: 5\t Loss: 0.6931471824645996\n",
            "Epoch: 6\t Loss: 0.787685215473175\n",
            "Epoch: 7\t Loss: 0.6931471824645996\n",
            "Epoch: 8\t Loss: 0.8156997561454773\n",
            "Epoch: 9\t Loss: 0.7955056428909302\n",
            "Epoch: 10\t Loss: 0.6931471824645996\n",
            "Epoch: 11\t Loss: 0.7881929278373718\n",
            "Epoch: 12\t Loss: 0.7460516691207886\n",
            "Epoch: 13\t Loss: 0.7962985634803772\n",
            "Epoch: 14\t Loss: 0.8654972314834595\n",
            "Epoch: 15\t Loss: 0.6931471824645996\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\victo\\OneDrive\\Desktop\\temp\\50.021-AI-Assignments\\HW5\\Week_6_AI_homework_neural_networks.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/victo/OneDrive/Desktop/temp/50.021-AI-Assignments/HW5/Week_6_AI_homework_neural_networks.ipynb#ch0000012?line=37'>38</a>\u001b[0m label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([train_labels\u001b[39m.\u001b[39mloc[j]])\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/victo/OneDrive/Desktop/temp/50.021-AI-Assignments/HW5/Week_6_AI_homework_neural_networks.ipynb#ch0000012?line=39'>40</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/victo/OneDrive/Desktop/temp/50.021-AI-Assignments/HW5/Week_6_AI_homework_neural_networks.ipynb#ch0000012?line=40'>41</a>\u001b[0m pred \u001b[39m=\u001b[39m model2(feats)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/victo/OneDrive/Desktop/temp/50.021-AI-Assignments/HW5/Week_6_AI_homework_neural_networks.ipynb#ch0000012?line=42'>43</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function(pred, label)\u001b[39m.\u001b[39mcuda() \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/victo/OneDrive/Desktop/temp/50.021-AI-Assignments/HW5/Week_6_AI_homework_neural_networks.ipynb#ch0000012?line=43'>44</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward() \n",
            "File \u001b[1;32mc:\\Users\\victo\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32mc:\\Users\\victo\\OneDrive\\Desktop\\temp\\50.021-AI-Assignments\\HW5\\Week_6_AI_homework_neural_networks.ipynb Cell 14'\u001b[0m in \u001b[0;36mMLP2.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/victo/OneDrive/Desktop/temp/50.021-AI-Assignments/HW5/Week_6_AI_homework_neural_networks.ipynb#ch0000012?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/victo/OneDrive/Desktop/temp/50.021-AI-Assignments/HW5/Week_6_AI_homework_neural_networks.ipynb#ch0000012?line=13'>14</a>\u001b[0m   out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/victo/OneDrive/Desktop/temp/50.021-AI-Assignments/HW5/Week_6_AI_homework_neural_networks.ipynb#ch0000012?line=14'>15</a>\u001b[0m   out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu(out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/victo/OneDrive/Desktop/temp/50.021-AI-Assignments/HW5/Week_6_AI_homework_neural_networks.ipynb#ch0000012?line=15'>16</a>\u001b[0m   out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(out)\n",
            "File \u001b[1;32mc:\\Users\\victo\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\victo\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# code your model 2\n",
        "\n",
        "class MLP2(nn.Module):\n",
        " \n",
        "  def __init__(self, input_size, num_classes):\n",
        "    super(MLP2, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size, 32)\n",
        "    self.fc2 = nn.Linear(32, 8)\n",
        "    self.fc3 = nn.Linear(8, num_classes)\n",
        "    self.dropout = nn.Dropout(p=0.75)\n",
        "   \n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.fc1(x)\n",
        "    out = torch.relu(out)\n",
        "    out = self.dropout(out)\n",
        "    out = self.fc2(out)\n",
        "    out = torch.relu(out)\n",
        "    out = self.fc3(out)\n",
        "    out = torch.relu(out)\n",
        "    out = torch.sigmoid(out)\n",
        "    return out\n",
        "\n",
        "# train model\n",
        "num_outputs = 1\n",
        "num_input_features = train_data.shape[1] - 1\n",
        "model2 = MLP2(num_input_features, num_outputs).cuda()\n",
        "\n",
        "lr_rate = 1e-4\n",
        "loss_function = nn.BCELoss() \n",
        "optimizer = torch.optim.SGD(model2.parameters(), lr=lr_rate, weight_decay=5e-4)\n",
        "\n",
        "epochs = 1000 \n",
        "\n",
        "for i in range(epochs):\n",
        "    for j in range(train_data.shape[0]):\n",
        "        feats = torch.tensor(train_feats.loc[j].values).float().cuda()\n",
        "        label = torch.tensor([train_labels.loc[j]]).float().cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = model2(feats)\n",
        "\n",
        "        loss = loss_function(pred, label).cuda() \n",
        "        loss.backward() \n",
        "        optimizer.step() \n",
        "\n",
        "    if i % 1 == 0:\n",
        "        print (f\"Epoch: {i}\\t Loss: {loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAIifiHJkHyW"
      },
      "outputs": [],
      "source": [
        "# evaluate model 2 (called model2 here)\n",
        "\n",
        "run_evaluation(model2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPsxbT0KkGs1"
      },
      "source": [
        "Which works better and why do you think this may be (very briefly)? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GzjI77HkSwH"
      },
      "source": [
        "**[your answer here, also please summarise the differences between your two models]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh5O8qS_khug"
      },
      "source": [
        "Additionally, submit your results [here](https://forms.gle/NtJJEE7Wm5ZRM3Je7) for 'Class specific accuracy of correctly predicting a hit song' and see if you got the best performance of the class! Good luck!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Week 6 - AI homework - neural networks",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('torch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "ef92e30b963217edf8c11f058cfcd3463da60128b43c8003f4b632a507dd4936"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
